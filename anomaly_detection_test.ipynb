{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa71f68",
   "metadata": {},
   "source": [
    "# Anomaly Detection Testing\n",
    "\n",
    "This notebook tests the percentage-based anomaly detection algorithm on wandb training data to verify it can detect specific anomalies in training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49719ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ab64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from CSV File\n",
    "df = pd.read_csv('wandb_runs_data.csv')\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Data Overview\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ff2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique runs to find our target runs\n",
    "print(\"Unique run names:\")\n",
    "unique_runs = df['run_name'].unique()\n",
    "for run in sorted(unique_runs):\n",
    "    print(f\"  {run}\")\n",
    "\n",
    "# Look for our specific target runs\n",
    "target_runs = [\n",
    "]\n",
    "\n",
    "print(f\"\\nTarget runs found:\")\n",
    "for target in target_runs:\n",
    "    matches = [run for run in unique_runs if target in run]\n",
    "    print(f\"  {target}: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43757dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the percentage-based anomaly detection function\n",
    "def detect_spikes_and_dips_local(data, spike_threshold=5.0, dip_threshold=5.0, min_distance=1, \n",
    "                                prominence_threshold=0.02, smoothing_window=2, local_window=3):\n",
    "    \"\"\"Detect spikes and dips using percentage comparison with neighboring points\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        values = np.array(data)\n",
    "        \n",
    "        if len(values) < 7:  # Need at least 7 points for 3 neighbors + center + 1 buffer\n",
    "            return {\n",
    "                'spike_indices': np.array([]),\n",
    "                'spike_values': np.array([]),\n",
    "                'dip_indices': np.array([]),\n",
    "                'dip_values': np.array([]),\n",
    "                'smoothed_data': values,\n",
    "                'local_stats': []\n",
    "            }\n",
    "        \n",
    "        # Apply smoothing (default window=2)\n",
    "        if smoothing_window > 1:\n",
    "            smoothed = np.convolve(values, np.ones(smoothing_window)/smoothing_window, mode='same')\n",
    "        else:\n",
    "            smoothed = values\n",
    "        \n",
    "        # Find spikes and dips using percentage comparison\n",
    "        spike_candidates = []\n",
    "        dip_candidates = []\n",
    "        local_stats = []\n",
    "        \n",
    "        # Use local_window as the number of neighboring points to compare (default=3)\n",
    "        neighbors = local_window\n",
    "        \n",
    "        for i in range(neighbors, len(smoothed) - neighbors):\n",
    "            # Get neighboring points (exclude the center point itself)\n",
    "            left_neighbors = smoothed[i-neighbors:i]\n",
    "            right_neighbors = smoothed[i+1:i+neighbors+1]\n",
    "            all_neighbors = np.concatenate([left_neighbors, right_neighbors])\n",
    "            \n",
    "            # Calculate average of neighboring points\n",
    "            neighbor_avg = np.mean(all_neighbors)\n",
    "            current_value = smoothed[i]\n",
    "            \n",
    "            # Store local stats for debugging\n",
    "            local_stats.append({\n",
    "                'index': i,\n",
    "                'current_value': current_value,\n",
    "                'neighbor_avg': neighbor_avg,\n",
    "                'neighbors': all_neighbors.tolist()\n",
    "            })\n",
    "            \n",
    "            # Check for spikes: current value is spike_threshold% higher than neighbor average\n",
    "            if abs(neighbor_avg) > 1e-10:  # Avoid division by zero with very small threshold\n",
    "                percent_increase = ((current_value - neighbor_avg) / abs(neighbor_avg)) * 100\n",
    "                if percent_increase >= spike_threshold:\n",
    "                    spike_candidates.append(i)\n",
    "            elif current_value > neighbor_avg:  # Handle zero/near-zero case\n",
    "                spike_candidates.append(i)\n",
    "            \n",
    "            # Check for dips: current value is dip_threshold% lower than neighbor average  \n",
    "            if abs(neighbor_avg) > 1e-10:  # Avoid division by zero with very small threshold\n",
    "                percent_decrease = ((neighbor_avg - current_value) / abs(neighbor_avg)) * 100\n",
    "                if percent_decrease >= dip_threshold:\n",
    "                    dip_candidates.append(i)\n",
    "            elif current_value < neighbor_avg:  # Handle zero/near-zero case\n",
    "                dip_candidates.append(i)\n",
    "        \n",
    "        # Simple distance filtering - keep all candidates but enforce minimum distance\n",
    "        spike_indices = []\n",
    "        if spike_candidates:\n",
    "            spike_candidates = sorted(spike_candidates)\n",
    "            spike_indices.append(spike_candidates[0])\n",
    "            for candidate in spike_candidates[1:]:\n",
    "                if candidate - spike_indices[-1] >= min_distance:\n",
    "                    spike_indices.append(candidate)\n",
    "        \n",
    "        dip_indices = []\n",
    "        if dip_candidates:\n",
    "            dip_candidates = sorted(dip_candidates)\n",
    "            dip_indices.append(dip_candidates[0])\n",
    "            for candidate in dip_candidates[1:]:\n",
    "                if candidate - dip_indices[-1] >= min_distance:\n",
    "                    dip_indices.append(candidate)\n",
    "        \n",
    "        return {\n",
    "            'spike_indices': np.array(spike_indices),\n",
    "            'spike_values': values[spike_indices] if spike_indices else np.array([]),\n",
    "            'dip_indices': np.array(dip_indices),\n",
    "            'dip_values': values[dip_indices] if dip_indices else np.array([]),\n",
    "            'smoothed_data': smoothed,\n",
    "            'local_stats': local_stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in anomaly detection: {str(e)}\")\n",
    "        return {\n",
    "            'spike_indices': np.array([]),\n",
    "            'spike_values': np.array([]),\n",
    "            'dip_indices': np.array([]),\n",
    "            'dip_values': np.array([]),\n",
    "            'smoothed_data': np.array(data),\n",
    "            'local_stats': []\n",
    "        }\n",
    "\n",
    "print(\"Anomaly detection function loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on  train/loss (should have peak around step 430)\n",
    "target_run = ''\n",
    "target_metric = 'train/loss'\n",
    "\n",
    "# Filter data for this specific run and metric\n",
    "test_data = df[(df['run_name'] == target_run) & (df['metric_name'] == target_metric)].copy()\n",
    "\n",
    "if len(test_data) > 0:\n",
    "    print(f\"Found {len(test_data)} data points for {target_run} {target_metric}\")\n",
    "    test_data = test_data.sort_values('step')\n",
    "    \n",
    "    # Get values and steps\n",
    "    steps = test_data['step'].values\n",
    "    values = test_data['value'].values\n",
    "    \n",
    "    print(f\"Step range: {steps.min()} to {steps.max()}\")\n",
    "    print(f\"Value range: {values.min():.6f} to {values.max():.6f}\")\n",
    "    \n",
    "    # Test anomaly detection\n",
    "    results = detect_spikes_and_dips_local(values, spike_threshold=5.0, dip_threshold=5.0)\n",
    "    \n",
    "    print(f\"\\nDetected {len(results['spike_indices'])} spikes at steps:\")\n",
    "    for idx in results['spike_indices']:\n",
    "        step = steps[idx]\n",
    "        value = values[idx]\n",
    "        print(f\"  Step {step}: {value:.6f}\")\n",
    "    \n",
    "    print(f\"\\nDetected {len(results['dip_indices'])} dips at steps:\")\n",
    "    for idx in results['dip_indices']:\n",
    "        step = steps[idx]\n",
    "        value = values[idx]\n",
    "        print(f\"  Step {step}: {value:.6f}\")\n",
    "        \n",
    "    # Check if we found the expected peak around step 430\n",
    "    expected_step = 430\n",
    "    found_near_430 = any(abs(steps[idx] - expected_step) <= 20 for idx in results['spike_indices'])\n",
    "    print(f\"\\nFound spike near step {expected_step}? {found_near_430}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"No data found for {target_run} {target_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on  (should have dip around step 2330)\n",
    "target_run2 = ''\n",
    "\n",
    "# Try to find any metric for this run (might not be train/loss)\n",
    "available_data = df[df['run_name'] == target_run2]\n",
    "\n",
    "if len(available_data) > 0:\n",
    "    print(f\"Available metrics for {target_run2}:\")\n",
    "    metrics = available_data['metric_name'].unique()\n",
    "    for metric in metrics:\n",
    "        count = len(available_data[available_data['metric_name'] == metric])\n",
    "        print(f\"  {metric}: {count} points\")\n",
    "    \n",
    "    # Test on the first available metric\n",
    "    test_metric = metrics[0]\n",
    "    test_data2 = available_data[available_data['metric_name'] == test_metric].copy()\n",
    "    test_data2 = test_data2.sort_values('step')\n",
    "    \n",
    "    steps2 = test_data2['step'].values\n",
    "    values2 = test_data2['value'].values\n",
    "    \n",
    "    print(f\"\\nTesting {target_run2} {test_metric}\")\n",
    "    print(f\"Step range: {steps2.min()} to {steps2.max()}\")\n",
    "    print(f\"Value range: {values2.min():.6f} to {values2.max():.6f}\")\n",
    "    \n",
    "    # Test anomaly detection\n",
    "    results2 = detect_spikes_and_dips_local(values2, spike_threshold=5.0, dip_threshold=5.0)\n",
    "    \n",
    "    print(f\"\\nDetected {len(results2['spike_indices'])} spikes at steps:\")\n",
    "    for idx in results2['spike_indices']:\n",
    "        step = steps2[idx]\n",
    "        value = values2[idx]\n",
    "        print(f\"  Step {step}: {value:.6f}\")\n",
    "    \n",
    "    print(f\"\\nDetected {len(results2['dip_indices'])} dips at steps:\")\n",
    "    for idx in results2['dip_indices']:\n",
    "        step = steps2[idx]\n",
    "        value = values2[idx]\n",
    "        print(f\"  Step {step}: {value:.6f}\")\n",
    "        \n",
    "    # Check if we found the expected dip around step 2330\n",
    "    expected_step2 = 2330\n",
    "    found_near_2330 = any(abs(steps2[idx] - expected_step2) <= 50 for idx in results2['dip_indices'])\n",
    "    print(f\"\\nFound dip near step {expected_step2}? {found_near_2330}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"No data found for {target_run2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de88f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first test case with plotly\n",
    "if len(test_data) > 0:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot original data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=steps,\n",
    "        y=values,\n",
    "        mode='lines',\n",
    "        name='Original Data',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    # Plot smoothed data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=steps,\n",
    "        y=results['smoothed_data'],\n",
    "        mode='lines',\n",
    "        name='Smoothed Data',\n",
    "        line=dict(color='lightblue', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    # Plot detected spikes\n",
    "    if len(results['spike_indices']) > 0:\n",
    "        spike_steps = steps[results['spike_indices']]\n",
    "        spike_values = values[results['spike_indices']]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=spike_steps,\n",
    "            y=spike_values,\n",
    "            mode='markers',\n",
    "            name='Detected Spikes',\n",
    "            marker=dict(color='red', size=10, symbol='triangle-up')\n",
    "        ))\n",
    "    \n",
    "    # Plot detected dips\n",
    "    if len(results['dip_indices']) > 0:\n",
    "        dip_steps = steps[results['dip_indices']]\n",
    "        dip_values = values[results['dip_indices']]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dip_steps,\n",
    "            y=dip_values,\n",
    "            mode='markers',\n",
    "            name='Detected Dips',\n",
    "            marker=dict(color='green', size=10, symbol='triangle-down')\n",
    "        ))\n",
    "    \n",
    "    # Highlight the expected area around step 430\n",
    "    fig.add_vline(x=430, line_dash=\"dot\", line_color=\"orange\", \n",
    "                  annotation_text=\"Expected Peak ~430\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{target_run} {target_metric} - Anomaly Detection Test',\n",
    "        xaxis_title='Step',\n",
    "        yaxis_title='Value',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No data to visualize\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
